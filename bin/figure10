#!/usr/bin/env python3

import argparse
import os
import csv
import pickle
import logging
from timeit import default_timer as timer

import numpy as np
import scipy
from fonduer import Meta, init_logging
from fonduer.candidates import CandidateExtractor, MentionExtractor
from fonduer.candidates.models import (
    Candidate,
    Mention,
    candidate_subclass,
    mention_subclass,
)
from fonduer.features import Featurizer
from fonduer.parser.models import Document, Figure, Paragraph, Section, Sentence
from fonduer.supervision import Labeler

from hack.transistors.transistor_lfs import ce_v_max_lfs, TRUE
from hack.transistors.transistor_matchers import get_matcher
from hack.transistors.transistor_spaces import MentionNgramsPart, MentionNgramsVolt
from hack.transistors.transistor_throttlers import ce_v_max_filter
from hack.transistors.transistor_utils import (
    get_gold_set,
    cand_to_entity,
    load_transistor_labels,
    candidates_to_entities,
    entity_level_scores,
)
from hack.utils import parse_dataset
from hack.transistors.transistors import generative_model, discriminative_model

logger = logging.getLogger(__name__)


def scoring(relation, disc_model, test_cands, test_docs, F_test, parts_by_doc, num=100):
    logger.info("Calculating the best F1 score and threshold (b)...")

    # Iterate over a range of `b` values in order to find the b with the
    # highest F1 score.
    Y_prob = disc_model.marginals((test_cands, F_test))

    for b in np.linspace(0, 1, num=num):
        try:
            test_score = np.array(
                [TRUE if p[TRUE - 1] > b else 3 - TRUE for p in Y_prob]
            )
            true_pred = [test_cands[_] for _ in np.nditer(np.where(test_score == TRUE))]
            result = entity_level_scores(
                candidates_to_entities(
                    true_pred, parts_by_doc=parts_by_doc, progress_bar=False
                ),
                attribute=relation,
                corpus=test_docs,
            )
            # yeild a CSV row
            yield [b, result.f1, result.prec, result.rec]

        except Exception as e:
            logger.debug(f"{e}, skipping.")
            break


def main(
    conn_string,
    max_docs=float("inf"),
    parse=False,
    first_time=False,
    re_label=False,
    gpu=None,
    parallel=4,
    log_dir=None,
    verbose=False,
):
    # Setup initial configuration
    if gpu:
        os.environ["CUDA_VISIBLE_DEVICES"] = gpu

    if not log_dir:
        log_dir = "logs"

    if verbose:
        level = logging.INFO
    else:
        level = logging.WARNING

    dirname = os.path.dirname(os.path.abspath(__file__))
    dirname = os.path.join(dirname, "../hack/transistors")
    init_logging(log_dir=os.path.join(dirname, log_dir), level=level)

    rel_list = ["ce_v_max"]

    session = Meta.init(conn_string).Session()

    # Parsing
    logger.info(f"Starting parsing...")
    start = timer()
    docs, train_docs, dev_docs, test_docs = parse_dataset(
        session, dirname, first_time=parse, parallel=parallel, max_docs=max_docs
    )
    end = timer()
    logger.warning(f"Parse Time (min): {((end - start) / 60.0):.1f}")

    logger.info(f"# of train Documents: {len(train_docs)}")
    logger.info(f"# of dev Documents: {len(dev_docs)}")
    logger.info(f"# of test Documents: {len(test_docs)}")
    logger.info(f"Documents: {session.query(Document).count()}")
    logger.info(f"Sections: {session.query(Section).count()}")
    logger.info(f"Paragraphs: {session.query(Paragraph).count()}")
    logger.info(f"Sentences: {session.query(Sentence).count()}")
    logger.info(f"Figures: {session.query(Figure).count()}")

    # Mention Extraction
    start = timer()
    mentions = []
    ngrams = []
    matchers = []

    # Only do those that are enabled
    Part = mention_subclass("Part")
    part_matcher = get_matcher("part")
    part_ngrams = MentionNgramsPart(parts_by_doc=None, n_max=3)

    mentions.append(Part)
    ngrams.append(part_ngrams)
    matchers.append(part_matcher)

    CeVMax = mention_subclass("CeVMax")
    ce_v_max_matcher = get_matcher("ce_v_max")
    ce_v_max_ngrams = MentionNgramsVolt(n_max=1)

    mentions.append(CeVMax)
    ngrams.append(ce_v_max_ngrams)
    matchers.append(ce_v_max_matcher)

    mention_extractor = MentionExtractor(session, mentions, ngrams, matchers)

    if first_time:
        mention_extractor.apply(docs, parallelism=parallel)

    logger.info(f"Total Mentions: {session.query(Mention).count()}")
    logger.info(f"Total Part: {session.query(Part).count()}")
    logger.info(f"Total CeVMax: {session.query(CeVMax).count()}")

    # Candidate Extraction
    cands = []
    throttlers = []

    PartCeVMax = candidate_subclass("PartCeVMax", [Part, CeVMax])
    ce_v_max_throttler = ce_v_max_filter

    cands.append(PartCeVMax)
    throttlers.append(ce_v_max_throttler)

    candidate_extractor = CandidateExtractor(session, cands, throttlers=throttlers)

    if first_time:
        for i, docs in enumerate([train_docs, dev_docs, test_docs]):
            candidate_extractor.apply(docs, split=i, parallelism=parallel)
            num_cands = session.query(Candidate).filter(Candidate.split == i).count()
            logger.info(f"Candidates in split={i}: {num_cands}")

    train_cands = candidate_extractor.get_candidates(split=0)
    dev_cands = candidate_extractor.get_candidates(split=1)
    test_cands = candidate_extractor.get_candidates(split=2)

    end = timer()
    logger.warning(f"Candidate Extraction Time (min): {((end - start) / 60.0):.1f}")

    logger.info(f"Total train candidate: {sum(len(_) for _ in train_cands)}")
    logger.info(f"Total dev candidate: {sum(len(_) for _ in dev_cands)}")
    logger.info(f"Total test candidate: {sum(len(_) for _ in test_cands)}")

    pickle_file = os.path.join(dirname, "data/parts_by_doc_new.pkl")
    with open(pickle_file, "rb") as f:
        parts_by_doc = pickle.load(f)

    # Check total recall
    for i, name in enumerate(rel_list):
        logger.info(name)
        result = entity_level_scores(
            candidates_to_entities(dev_cands[i], parts_by_doc=parts_by_doc),
            attribute=name,
            corpus=dev_docs,
        )
        logger.info(f"{name} Total Dev Recall: {result.rec:.3f}")
        result = entity_level_scores(
            candidates_to_entities(test_cands[i], parts_by_doc=parts_by_doc),
            attribute=name,
            corpus=test_docs,
        )
        logger.info(f"{name} Total Test Recall: {result.rec:.3f}")

    # Featurization
    start = timer()
    cands = [PartCeVMax]

    featurizer = Featurizer(session, cands)
    if first_time:
        logger.info("Starting featurizer...")
        featurizer.apply(split=0, train=True, parallelism=parallel)
        featurizer.apply(split=1, parallelism=parallel)
        featurizer.apply(split=2, parallelism=parallel)
        logger.info("Done")

    logger.info("Getting feature matrices...")
    if first_time:
        F_train = featurizer.get_feature_matrices(train_cands)
        F_dev = featurizer.get_feature_matrices(dev_cands)
        F_test = featurizer.get_feature_matrices(test_cands)
        end = timer()
        logger.warning(f"Featurization Time (min): {((end - start) / 60.0):.1f}")

        pickle.dump(F_train, open(os.path.join(dirname, "F_train.pkl"), "wb"))
        pickle.dump(F_dev, open(os.path.join(dirname, "F_dev.pkl"), "wb"))
        pickle.dump(F_test, open(os.path.join(dirname, "F_test.pkl"), "wb"))
    else:
        F_train = pickle.load(open(os.path.join(dirname, "F_train.pkl"), "rb"))
        F_dev = pickle.load(open(os.path.join(dirname, "F_dev.pkl"), "rb"))
        F_test = pickle.load(open(os.path.join(dirname, "F_test.pkl"), "rb"))
    logger.info("Done.")

    for i, cand in enumerate(cands):
        logger.info(f"{cand} Train shape: {F_train[i].shape}")
        logger.info(f"{cand} Test shape: {F_test[i].shape}")
        logger.info(f"{cand} Dev shape: {F_dev[i].shape}")

    logger.info("Labeling training data...")

    # Labeling
    start = timer()
    lfs = [ce_v_max_lfs]

    labeler = Labeler(session, cands)

    if first_time:
        logger.info("Applying LFs...")
        labeler.apply(split=0, lfs=lfs, train=True, parallelism=parallel)
        logger.info("Done...")

        # Uncomment if debugging LFs
        load_transistor_labels(session, cands, ["ce_v_max"])
        labeler.apply(split=1, lfs=lfs, train=False, parallelism=parallel)
        #  labeler.apply(split=2, lfs=lfs, train=False, parallelism=parallel)

    elif re_label:
        logger.info("Updating LFs...")
        labeler.update(split=0, lfs=lfs, parallelism=parallel)
        logger.info("Done...")

        # Uncomment if debugging LFs
        labeler.apply(split=1, lfs=lfs, train=False, parallelism=parallel)
        #  labeler.apply(split=2, lfs=lfs, train=False, parallelism=parallel)

    logger.info("Getting label matrices...")

    L_train = labeler.get_label_matrices(train_cands)

    # Uncomment if debugging LFs
    L_dev = labeler.get_label_matrices(dev_cands)
    #  L_dev_gold = labeler.get_gold_labels(dev_cands, annotator="gold")
    #  L_test = labeler.get_label_matrices(test_cands)
    #  L_test_gold = labeler.get_gold_labels(test_cands, annotator="gold")

    logger.info("Done.")

    end = timer()
    logger.warning(f"Supervision Time (min): {((end - start) / 60.0):.1f}")

    start = timer()

    relation = "ce_v_max"
    idx = rel_list.index(relation)

    # Can be uncommented for use in debugging labeling functions
    #  logger.info("Updating labeling function summary...")
    #  keys = labeler.get_keys()
    #  logger.info("Summary for train set labeling functions:")
    #  df = analysis.lf_summary(L_train[idx], lf_names=keys)
    #  logger.info(f"\n{df.to_string()}")
    #
    #  logger.info("Summary for dev set labeling functions:")
    #  df = analysis.lf_summary(
    #      L_dev[idx],
    #      lf_names=keys,
    #      Y=L_dev_gold[idx].todense().reshape(-1).tolist()[0],
    #  )
    #  logger.info(f"\n{df.to_string()}")
    #
    #  logger.info("Summary for test set labeling functions:")
    #  df = analysis.lf_summary(
    #      L_test[idx],
    #      lf_names=keys,
    #      Y=L_test_gold[idx].todense().reshape(-1).tolist()[0],
    #  )
    #  logger.info(f"\n{df.to_string()}")

    # Evaluate using human annotations
    dev_gold_entities = get_gold_set(attribute=relation)
    marginals_human = []
    for c in dev_cands[idx]:
        if cand_to_entity(c) in dev_gold_entities:
            marginals_human.append([0.0, 1.0])
        else:
            marginals_human.append([1.0, 0.0])
    marginals_human = np.array(marginals_human)

    disc_model_ce_v_max = discriminative_model(
        dev_cands[idx], F_dev[idx], marginals_human, n_epochs=100, gpu=gpu
    )

    outfile = "fig10.csv"
    with open(outfile, "w") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(("type", "b", "f1", "precision", "recall"))

    logger.info("Score of human annotations")
    rows = scoring(
        relation,
        disc_model_ce_v_max,
        test_cands[idx],
        test_docs,
        F_test[idx],
        parts_by_doc,
        num=100,
    )

    with open(outfile, "a") as csvfile:
        writer = csv.writer(csvfile)
        for row in rows:
            writer.writerow(["human"] + row)

    # N x 2 ndarray
    marginals_train = generative_model(L_train[idx])
    marginals_dev = generative_model(L_dev[idx])

    combined_cands = train_cands[idx] + dev_cands[idx]
    F_combined = scipy.sparse.vstack((F_train[idx], F_dev[idx]))
    marginals_combined = np.vstack((marginals_train, marginals_dev))

    disc_model_ce_v_max = discriminative_model(
        combined_cands, F_combined, marginals_combined, n_epochs=100, gpu=gpu
    )

    logger.info("Score of heuristics")
    rows = scoring(
        relation,
        disc_model_ce_v_max,
        test_cands[idx],
        test_docs,
        F_test[idx],
        parts_by_doc,
        num=100,
    )
    with open(outfile, "a") as csvfile:
        writer = csv.writer(csvfile)
        for row in rows:
            writer.writerow(["heuristics"] + row)

    marginals_combined = np.vstack((marginals_train, marginals_human))

    disc_model_ce_v_max = discriminative_model(
        combined_cands, F_combined, marginals_combined, n_epochs=100, gpu=gpu
    )

    # Can be uncommented to view score on development set
    #  best_result, best_b = scoring(
    #      relation,
    #      disc_model_ce_v_max,
    #      dev_cands[idx],
    #      dev_docs,
    #      F_dev[idx],
    #      parts_by_doc,
    #      num=100,
    #  )

    logger.info("Score of combined")
    rows = scoring(
        relation,
        disc_model_ce_v_max,
        test_cands[idx],
        test_docs,
        F_test[idx],
        parts_by_doc,
        num=100,
    )
    with open(outfile, "a") as csvfile:
        writer = csv.writer(csvfile)
        for row in rows:
            writer.writerow(["combined"] + row)

    end = timer()
    logger.warning(f"Classification Time (min): {((end - start) / 60.0):.1f}")
    logger.warning(f"File saved to {outfile}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Commandline interface for KBC for transistors."
    )
    parser.add_argument("--parse", action="store_true", help="Parse the dataset.")
    parser.add_argument(
        "--first-time",
        help="Run all stages of the pipeline (except parsing).",
        action="store_true",
    )
    parser.add_argument(
        "--re-label",
        help="Re-run weak supervision (assumes rest of pipeline has been done).",
        action="store_true",
    )
    parser.add_argument(
        "--max-docs",
        type=int,
        default=float("inf"),
        help="The number of docs to parse from (dev/test/train). Defaults to all docs.",
    )
    parser.add_argument(
        "--parallel", type=int, default=8, help="Set the level of parallelization."
    )
    parser.add_argument("--gpu", type=str, help="Use the specified GPU index.")
    parser.add_argument(
        "--conn-string", type=str, help="Connection string to the PosgreSQL Database."
    )
    parser.add_argument("--log-dir", type=str, help="Directory to output log files.")
    parser.add_argument("-v", help="Set INFO level logging.", action="store_true")

    args = parser.parse_args()

    main(
        args.conn_string,
        max_docs=args.max_docs,
        parse=args.parse,
        first_time=args.first_time,
        re_label=args.re_label,
        gpu=args.gpu,
        parallel=args.parallel,
        log_dir=args.log_dir,
        verbose=args.v,
    )
